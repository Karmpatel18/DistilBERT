{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8872405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizer\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import joblib\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertModel\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1508c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    An TLS 1.2 connection request was received fro...\n",
       "1    Application popup: Idle timer expired : Sessio...\n",
       "2    The storage optimizer successfully completed r...\n",
       "3    A new process has been created.Creator Subject...\n",
       "4    Key file operation.Subject: Security ID: S-1-5...\n",
       "Name: event_text, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing data file and creating dataframe\n",
    "df = pd.read_csv(\"train_data.csv\", on_bad_lines='skip')\n",
    "df[\"event_text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"event_text\"] = df[\"event_text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957dfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical labels (for each field)\n",
    "\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "for col in [\"eventClass\", \"eventDeviceCat\", \"eventOperation\", \"eventOutcome\"]:\n",
    "    enc = LabelEncoder()\n",
    "    df[col] = enc.fit_transform(df[col].astype(str)) \n",
    "    encoders[col] = enc \n",
    "\n",
    "df[\"eventSeverity\"] = df[\"eventSeverity\"].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"event_text\"].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e8cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_id           int64\n",
      "event_text        object\n",
      "eventClass         int64\n",
      "eventDeviceCat     int64\n",
      "eventOperation     int64\n",
      "eventOutcome       int64\n",
      "eventSeverity      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.encodings = tokenizer(list(df[\"event_text\"]), padding=True, truncation=True, max_length=128)\n",
    "        self.labels = df[[\"eventClass\", \"eventDeviceCat\", \"eventOperation\", \"eventOutcome\", \"eventSeverity\"]].values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c62dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = LogDataset(train_df, tokenizer)\n",
    "val_dataset = LogDataset(val_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputDistilBERT(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(MultiOutputDistilBERT, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        # Create one classifier head per field\n",
    "        self.classifier_class = nn.Linear(hidden_size, len(df[\"eventClass\"].unique()))\n",
    "        self.classifier_device = nn.Linear(hidden_size, len(df[\"eventDeviceCat\"].unique()))\n",
    "        self.classifier_operation = nn.Linear(hidden_size, len(df[\"eventOperation\"].unique()))\n",
    "        self.classifier_outcome = nn.Linear(hidden_size, len(df[\"eventOutcome\"].unique()))\n",
    "        self.classifier_severity = nn.Linear(hidden_size, 6)  # assuming severity 1–5\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        return {\n",
    "            \"eventClass\": self.classifier_class(pooled_output),\n",
    "            \"eventDeviceCat\": self.classifier_device(pooled_output),\n",
    "            \"eventOperation\": self.classifier_operation(pooled_output),\n",
    "            \"eventOutcome\": self.classifier_outcome(pooled_output),\n",
    "            \"eventSeverity\": self.classifier_severity(pooled_output)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6bf152",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc1036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputDistilBERT(\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier_class): Linear(in_features=768, out_features=9, bias=True)\n",
       "  (classifier_device): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (classifier_operation): Linear(in_features=768, out_features=8, bias=True)\n",
       "  (classifier_outcome): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (classifier_severity): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model init\n",
    "model = MultiOutputDistilBERT(\"distilbert-base-uncased\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bd8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 6.8198\n",
      "Epoch 2 | Loss: 5.6404\n",
      "Epoch 3 | Loss: 4.9196\n",
      "Epoch 4 | Loss: 4.1138\n",
      "Epoch 5 | Loss: 3.4601\n",
      "Epoch 6 | Loss: 2.7350\n",
      "Epoch 7 | Loss: 2.2702\n",
      "Epoch 8 | Loss: 1.9231\n",
      "Epoch 9 | Loss: 1.5582\n",
      "Epoch 10 | Loss: 1.3902\n",
      "Epoch 11 | Loss: 1.1912\n",
      "Epoch 12 | Loss: 0.9551\n",
      "Epoch 13 | Loss: 0.8061\n",
      "Epoch 14 | Loss: 0.6942\n",
      "Epoch 15 | Loss: 0.5996\n"
     ]
    }
   ],
   "source": [
    "# Training loop with 15 epoch\n",
    "model.train()\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = (\n",
    "            F.cross_entropy(outputs[\"eventClass\"], labels[:, 0].long()) +\n",
    "            F.cross_entropy(outputs[\"eventDeviceCat\"], labels[:, 1].long()) +\n",
    "            F.cross_entropy(outputs[\"eventOperation\"], labels[:, 2].long()) +\n",
    "            F.cross_entropy(outputs[\"eventOutcome\"], labels[:, 3].long()) +\n",
    "            F.cross_entropy(outputs[\"eventSeverity\"], labels[:, 4].long())\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04701d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trained model saved as 'Model/multioutput_distilbert.pt'\n"
     ]
    }
   ],
   "source": [
    "# Create directory to store model\n",
    "os.makedirs(\"Model\", exist_ok=True)\n",
    "\n",
    "# Save model weights (entire state_dict)\n",
    "torch.save(model.state_dict(), \"Model/multioutput_distilbert.pt\", _use_new_zipfile_serialization=True)\n",
    "\n",
    "print(\"✅ Trained model saved as 'Model/multioutput_distilbert.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c74a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer and label encoders saved too.\n"
     ]
    }
   ],
   "source": [
    "# Saving tokenizer and label encoders\n",
    "tokenizer.save_pretrained(\"Model/\")\n",
    "\n",
    "\n",
    "for field, enc in encoders.items():\n",
    "    joblib.dump(enc, f\"Model/{field}_encoder.pkl\")\n",
    "\n",
    "print(\"✅ Tokenizer and label encoders saved too.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e72e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.340741276741028\n",
      "✅ Predictions with confidence saved to output_with_confidence.json\n",
      "✅ Logs stored in prediction_logs.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Validating data and Logging prediction outputs \n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"prediction_logs.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        \n",
    "        # Calculate loss\n",
    "        \n",
    "        loss = (\n",
    "            F.cross_entropy(outputs[\"eventClass\"], labels[:, 0].long()) +\n",
    "            F.cross_entropy(outputs[\"eventDeviceCat\"], labels[:, 1].long()) +\n",
    "            F.cross_entropy(outputs[\"eventOperation\"], labels[:, 2].long()) +\n",
    "            F.cross_entropy(outputs[\"eventOutcome\"], labels[:, 3].long()) +\n",
    "            F.cross_entropy(outputs[\"eventSeverity\"], labels[:, 4].long())\n",
    "        )\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        \n",
    "        # Get predictions & confidence\n",
    "        \n",
    "        softmax_outputs = {\n",
    "            k: F.softmax(v, dim=1) for k, v in outputs.items()\n",
    "        }\n",
    "\n",
    "        preds = {k: torch.argmax(v, dim=1).cpu().numpy() for k, v in outputs.items()}\n",
    "        confs = {k: torch.max(softmax_outputs[k], dim=1).values.cpu().numpy() for k in outputs}\n",
    "\n",
    "        \n",
    "        # Store results + log\n",
    "        \n",
    "        for i in range(len(preds[\"eventClass\"])):\n",
    "            event_id = int(batch_idx * val_loader.batch_size + i + 1)\n",
    "            result = {\n",
    "                \"event_id\": event_id,\n",
    "                \"eventClass\": encoders[\"eventClass\"].inverse_transform([preds[\"eventClass\"][i]])[0],\n",
    "                \"eventDeviceCat\": encoders[\"eventDeviceCat\"].inverse_transform([preds[\"eventDeviceCat\"][i]])[0],\n",
    "                \"eventOperation\": encoders[\"eventOperation\"].inverse_transform([preds[\"eventOperation\"][i]])[0],\n",
    "                \"eventOutcome\": encoders[\"eventOutcome\"].inverse_transform([preds[\"eventOutcome\"][i]])[0],\n",
    "                \"eventSeverity\": int(preds[\"eventSeverity\"][i]),\n",
    "                \"confidence\": {\n",
    "                    \"eventClass\": float(confs[\"eventClass\"][i]),\n",
    "                    \"eventDeviceCat\": float(confs[\"eventDeviceCat\"][i]),\n",
    "                    \"eventOperation\": float(confs[\"eventOperation\"][i]),\n",
    "                    \"eventOutcome\": float(confs[\"eventOutcome\"][i]),\n",
    "                    \"eventSeverity\": float(confs[\"eventSeverity\"][i]),\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Log info\n",
    "            logging.info(f\"Event {event_id}: {json.dumps(result)}\")\n",
    "\n",
    "\n",
    "print(\"Validation Loss:\", val_loss / len(val_loader))\n",
    "with open(\"output_with_confidence.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"✅ Predictions with confidence saved to output_with_confidence.json\")\n",
    "print(\"✅ Logs stored in prediction_logs.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60efd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
